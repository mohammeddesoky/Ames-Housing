# ğŸ“˜ Linear Regression Models Comparison

This project demonstrates and compares different types of **Linear Regression models** using Python and scikit-learn.  
It includes data preprocessing, model training, evaluation (using RÂ² and RMSE), and performance comparison for all models.

---

## ğŸš€ Models Covered

### 1ï¸âƒ£ Simple Linear Regression
- **Description:** Models the relationship between a single independent variable (X) and a dependent variable (y).
- **Formula:** `y = b0 + b1*x`
- **Regularization:** âŒ None  
- **Use Case:** When there is only one feature affecting the target variable.

---

### 2ï¸âƒ£ Multiple Linear Regression
- **Description:** Extends simple linear regression to multiple features.
- **Formula:** `y = b0 + b1*x1 + b2*x2 + ... + bn*xn`
- **Regularization:** âŒ None  
- **Use Case:** When several independent variables influence the target.

---

### 3ï¸âƒ£ Ridge Regression (L2 Regularization)
- **Description:** A linear model that adds a penalty equal to the **square of the magnitude of coefficients**.  
- **Penalty Term:** `Î± * Î£(biÂ²)`
- **Effect:** Shrinks large coefficients but does **not** make them zero.  
- **Regularization:** âœ… L2  
- **Use Case:** When the dataset has multicollinearity or risk of overfitting.

---

### 4ï¸âƒ£ Lasso Regression (L1 Regularization)
- **Description:** Adds a penalty equal to the **absolute value of the magnitude of coefficients**.  
- **Penalty Term:** `Î± * Î£|bi|`
- **Effect:** Can shrink some coefficients completely to **zero**, effectively performing **feature selection**.  
- **Regularization:** âœ… L1  
- **Use Case:** When you want both regularization and automatic feature selection.

---

### 5ï¸âƒ£ Elastic Net Regression
- **Description:** Combines both L1 (Lasso) and L2 (Ridge) penalties.  
- **Penalty Term:** `Î± * (l1_ratio * Î£|bi| + (1 - l1_ratio) * Î£(biÂ²))`
- **Effect:** Balances between Ridgeâ€™s stability and Lassoâ€™s feature selection.  
- **Regularization:** âœ… L1 + L2  
- **Use Case:** When features are correlated and both shrinkage & selection are important.

---

## âš™ï¸ Workflow

1. **Data Preprocessing**
   - Handle missing values (`dropna`, `fillna`)
   - Remove outliers using boxplots
   - Encode categorical variables (One-Hot Encoding)
   - Feature scaling (`StandardScaler`)

2. **Model Training**
   - Train each regression model separately using scikit-learn.

3. **Evaluation Metrics**
   - **RÂ² Score:** Measures how well the model explains the variance in data.
   - **RMSE (Root Mean Squared Error):** Measures prediction error magnitude.

4. **Comparison**
   - Display RÂ² and RMSE results for all models in a single table to compare performance.

---

## ğŸ§ª Experiment Results

The models were trained and evaluated on the **House Prices (Diabetes/Custom)** dataset using the same preprocessing and train-test split.  
Below are the actual performance results obtained from this project:

### ğŸ“Š Performance Comparison

| Model | RÂ² Score | RMSE |
|--------|-----------|--------|
| Linear Regression | 0.894077 | 29,141.76 |
| Ridge Regression | 0.894491 | 29,084.80 |
| Lasso Regression | 0.894816 | 29,039.89 |
| Elastic Net Regression | **0.894855** | **29,034.49** |

### ğŸ§  Interpretation
- All models performed quite similarly, showing a strong linear relationship.  
- **Elastic Net Regression** achieved the **highest RÂ² Score** and the **lowest RMSE**, indicating slightly better performance.  
- The improvement margins are small, suggesting the dataset fits well with standard Linear Regression, but **regularization** still helps fine-tune stability and reduce overfitting.

---

## ğŸ§  Key Takeaways
- **Ridge** helps reduce overfitting.
- **Lasso** can remove irrelevant features.
- **Elastic Net** combines both advantages.
- Always **scale your data** before applying any regularized regression model.

---

## ğŸ§© Technologies Used
- Python ğŸ  
- pandas, numpy  
- scikit-learn  
- matplotlib, seaborn  

---
